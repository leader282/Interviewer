{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "krvnkOEewVpE",
        "outputId": "ad079d38-dc8c-47ff-8a89-1aa02a9c6a0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Processing Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 348.25it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671d65ae-39f2305f40b095013fb8c95c;48fdc100-fc68-4745-a231-4fb702288c46)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=1.4189856847127278, metrics={'train_runtime': 14.2219, 'train_samples_per_second': 1.055, 'train_steps_per_second': 0.211, 'total_flos': 44921090211840.0, 'train_loss': 1.4189856847127278, 'epoch': 3.0})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "\n",
        "# Load the tokenizer and model (LLaMA 2-3B from Hugging Face)\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "hf_token = \"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\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)\n",
        "\n",
        "# Add a padding token to the tokenizer if it doesn't have one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # For causal language modeling\n",
        "    r=16,                          # Rank of the LoRA updates\n",
        "    lora_alpha=32,                 # Scaling factor for LoRA\n",
        "    lora_dropout=0.1,              # Dropout for LoRA layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Set up training configurations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-llama-2-3b-unsupervised\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=[],\n",
        "    save_safetensors=False\n",
        ")\n",
        "\n",
        "# Prepare a simple self-supervised dataset for unsupervised fine-tuning\n",
        "# Assuming you have raw text data in a file named 'data.txt' for simplicity\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "dataset = pd.read_csv('interviewer.csv', encoding='Windows-1252')\n",
        "\n",
        "# Tokenize the dataset for LLaMA (causal language modeling task)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tqdm.pandas(desc=\"Processing Data\")\n",
        "tokenized_datasets = dataset.progress_apply(lambda x: tokenize_function(x), axis=1)\n",
        "\n",
        "# Define data collator for causal LM (shifts labels for next token prediction)\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialize Trainer for LoRA fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptIA6ZkNyYPx",
        "outputId": "cc7c5e78-86ea-4834-8679-6fec9ecb740d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671d65af-5bb5035c09c0a5b81e2e8448;60f06a3e-e0d8-47e0-ab7a-3221dacf36e9)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./lora-llama-2-3b-unsupervised/tokenizer_config.json',\n",
              " './lora-llama-2-3b-unsupervised/special_tokens_map.json',\n",
              " './lora-llama-2-3b-unsupervised/tokenizer.json')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./lora-llama-2-3b-unsupervised\")\n",
        "tokenizer.save_pretrained(\"./lora-llama-2-3b-unsupervised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWjPKFZe1p_T",
        "outputId": "dcd5eaf5-a220-4cf8-a92f-fe43cebf0c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: lora-llama-2-3b-unsupervised/ (stored 0%)\n",
            "  adding: lora-llama-2-3b-unsupervised/adapter_model.safetensors (deflated 10%)\n",
            "  adding: lora-llama-2-3b-unsupervised/README.md (deflated 66%)\n",
            "  adding: lora-llama-2-3b-unsupervised/tokenizer_config.json (deflated 94%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/ (stored 0%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/README.md (deflated 66%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/optimizer.pt (deflated 7%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/training_args.bin (deflated 51%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/scheduler.pt (deflated 56%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/trainer_state.json (deflated 55%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/rng_state.pth (deflated 25%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/adapter_config.json (deflated 50%)\n",
            "  adding: lora-llama-2-3b-unsupervised/checkpoint-3/adapter_model.bin (deflated 10%)\n",
            "  adding: lora-llama-2-3b-unsupervised/special_tokens_map.json (deflated 70%)\n",
            "  adding: lora-llama-2-3b-unsupervised/tokenizer.json (deflated 74%)\n",
            "  adding: lora-llama-2-3b-unsupervised/adapter_config.json (deflated 50%)\n"
          ]
        }
      ],
      "source": [
        "! zip -r lora-llama-2-3b-unsupervised.zip lora-llama-2-3b-unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zycXTfDU2g9v",
        "outputId": "153d2fab-faf2-4d07-f814-710729390ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interviewer: To start, can you describe how Django manages user sessions?\n",
            "Interviewee: yes\n",
            "Interviewer: Can you explain how Django handles session data\n",
            "Interviewee: no i can't\n",
            "Interviewer: Can you describe the difference between a session\n",
            "Interviewee: none\n",
            "Interviewer: Can you tell me about the role of\n",
            "Interviewee: nothing\n",
            "Interviewer: How do you handle session expiration?\n",
            "Interview\n",
            "Interviewee: i can't\n",
            "Interviewer: Can you provide an example of how to\n",
            "Interviewee: no baby\n",
            "Interviewer: In this case, I would like to\n",
            "Interviewee: what\n",
            "Interviewer: 2.3.4.5\n",
            "Interviewee: trtr\n",
            "Interviewer: 3.2.5.6\n",
            "Interviewee: ask more question\n",
            "Interviewer: 4.4\n",
            "Interviewe\n",
            "Interview\n",
            "Interviewee: exit\n",
            "Interviewer: 6\n",
            "This interview was a disaster.\n",
            "Interviewer: Thank you for your time. This concludes the interview.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"./lora-llama-2-3b-unsupervised\"  # Path to the fine-tuned model directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function to simulate the interview\n",
        "def interview_simulation(model, tokenizer):\n",
        "    # Initial context to set the tone for the interview\n",
        "    context = \"Interviewee: Hello, I'm ready for the interview.\\nInterviewer: Welcome! Let's start with some questions about Django and Flask.\\n\"\n",
        "\n",
        "    # First question to start the interview\n",
        "    interviewer_question = \"To start, can you describe how Django manages user sessions?\"\n",
        "    print(f\"Interviewer: {interviewer_question}\")\n",
        "    context += f\"Interviewer: {interviewer_question}\\n\"\n",
        "\n",
        "    # Start the interactive loop\n",
        "    while True:\n",
        "        torch.cuda.empty_cache()\n",
        "        # Interviewee's input\n",
        "        Interviewee_response = input(\"Interviewee: \")\n",
        "        context += f\"Interviewee: {Interviewee_response}\\n\"\n",
        "\n",
        "        # Prepare context for next interviewer question\n",
        "        context += \"Interviewer: \"\n",
        "\n",
        "        # Tokenize and generate the next question based on the context\n",
        "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                # max_length=512,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                attention_mask=attention_mask,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,\n",
        "                max_new_tokens=8,\n",
        "                temperature=0.7  # Control creativity for follow-ups\n",
        "            )\n",
        "\n",
        "        # Decode the generated question\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        next_interviewer_question = generated_text.split(\"Interviewer:\")[-1].strip().split(\"Interviewee:\")[0].strip()\n",
        "\n",
        "        # Print and update context\n",
        "        print(f\"Interviewer: {next_interviewer_question}\")\n",
        "        context += f\"{next_interviewer_question}\\n\"\n",
        "\n",
        "        # Exit condition for the loop\n",
        "        if Interviewee_response.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Interviewer: Thank you for your time. This concludes the interview.\")\n",
        "            break\n",
        "\n",
        "# Run the interview simulation\n",
        "interview_simulation(model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

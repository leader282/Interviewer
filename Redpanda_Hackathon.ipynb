{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "krvnkOEewVpE",
        "outputId": "ad079d38-dc8c-47ff-8a89-1aa02a9c6a0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4788b033d984ab4813b4edf9bd67078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7621ae1866f24e3db6f166d9ab7e9d76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 118.9245, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.042, 'train_loss': 3.0918622970581056, 'epoch': 3.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671d7894-06c8e9331963f2e50a9315ef;563c8226-e598-4cb1-b15b-2e98cf9eb745)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "  warnings.warn(\n",
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=3.0918622970581056, metrics={'train_runtime': 118.9245, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.042, 'total_flos': 101821137813504.0, 'train_loss': 3.0918622970581056, 'epoch': 3.3333333333333335})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the tokenizer and model (LLaMA 2-3B from Hugging Face)\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "hf_token = \"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\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token).half().to('cuda')\n",
        "\n",
        "# Add a padding token to the tokenizer if it doesn't have one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # For causal language modeling\n",
        "    r=16,                          # Rank of the LoRA updates\n",
        "    lora_alpha=32,                 # Scaling factor for LoRA\n",
        "    lora_dropout=0.1,              # Dropout for LoRA layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Set up training configurations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-llama-2-3b-unsupervised\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=[],\n",
        "    save_safetensors=False\n",
        ")\n",
        "\n",
        "# Sample dataset for demonstration\n",
        "dataset = [\n",
        "    {\n",
        "        \"prompt\": \"Let's start with Django. How does Django manage user sessions?\",\n",
        "        \"response\": \"Django uses a session framework to store data on a per-user basis. It generates a session ID that's stored in the user's browser, while the associated session data is stored on the server, typically in the database or a cache.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Good. Can you elaborate on how Django uses middleware in managing sessions?\",\n",
        "        \"response\": \"Sure. Middleware in Django processes requests before they reach the view and after they've been processed by the view. For sessions, Django has middleware that reads the session data from the request and loads it, then saves any changes to the session before sending the response back to the client.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"That's correct. Now, explain how the Django ORM simplifies interactions with the database.\",\n",
        "        \"response\": \"The Django ORM lets developers interact with the database using Python code instead of writing raw SQL. It provides models that map directly to database tables, which makes querying, creating, and updating records easy and database-agnostic.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What are some limitations or challenges with using Django ORM?\",\n",
        "        \"response\": \"One challenge is with complex queries. While Django ORM covers most CRUD operations well, it can become less efficient for complex joins or aggregations. In those cases, developers sometimes need to use raw SQL for better performance.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"That makes sense. Moving to Flask, what are blueprints and how would they benefit a larger application?\",\n",
        "        \"response\": \"Blueprints in Flask help organize application functionality by allowing different sections of an app to be separated into modular components. For example, in a large app, you could have separate blueprints for authentication, user management, and content, which keeps the codebase more organized and scalable.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"If we use Flask for an API-based application, how would you handle HTTP methods and route organization?\",\n",
        "        \"response\": \"In Flask, you can specify HTTP methods directly on routes by setting the `methods` argument. For example, using `@app.route('/login', methods=['POST'])` ensures that the route only accepts POST requests. Routes can be further organized by grouping them within blueprints, which keeps related routes in one place.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Let's discuss environments. How can Flask manage different environments like development and production?\",\n",
        "        \"response\": \"Flask can handle multiple environments through configuration objects or files. By setting the `FLASK_ENV` environment variable to 'development' or 'production', you can load different settings, such as enabling debug mode in development and setting specific database URIs for each environment.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Can you explain Django signals and provide an example of when they'd be useful in a project?\",\n",
        "        \"response\": \"Django signals are used to trigger actions when certain events occur in the application. For instance, you might use a signal to create a user profile automatically after a new user registers. This decouples the user creation process from the profile creation process, which keeps the code modular.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Great example. Now, how would you approach file uploads in Flask?\",\n",
        "        \"response\": \"In Flask, file uploads are handled through `request.files`. First, you configure an upload folder using `app.config['UPLOAD_FOLDER']`, then access the file via `request.files['file']`. Finally, you can save it to the server using the `save` method.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Lastly, in both Django and Flask, how would you implement user authentication in a custom way?\",\n",
        "        \"response\": \"In Django, custom authentication can be done by creating a custom user model and setting `AUTH_USER_MODEL` in `settings.py`. In Flask, custom authentication typically involves creating a login route that checks credentials against a database, using JWT tokens or sessions to manage authenticated states.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Function to prepare dataset for training\n",
        "def prepare_dataset(dataset):\n",
        "    texts = []\n",
        "    for entry in dataset:\n",
        "        text = f\"Interviewer: {entry['prompt']}\\nCandidate: {entry['response']}\\n\"\n",
        "        texts.append(text)\n",
        "    return Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "hf_dataset = prepare_dataset(dataset)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define data collator for causal LM (shifts labels for next token prediction)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialize Trainer for LoRA fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671d78aa-1c6d46e73961563f12630bcc;d722dd5c-4c31-4b4a-9ddb-82fe70021a4b)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "  warnings.warn(\n",
            "c:\\Users\\chakr\\Desktop\\RedPanda\\env\\Lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./lora-llama-2-3b-unsupervised\\\\tokenizer_config.json',\n",
              " './lora-llama-2-3b-unsupervised\\\\special_tokens_map.json',\n",
              " './lora-llama-2-3b-unsupervised\\\\tokenizer.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained('./lora-llama-2-3b-unsupervised')\n",
        "tokenizer.save_pretrained('./lora-llama-2-3b-unsupervised')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zycXTfDU2g9v",
        "outputId": "153d2fab-faf2-4d07-f814-710729390ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interviewer: To start, can you describe how Django manages user sessions?\n",
            "Interviewee: I guess its middleware\n",
            "Interviewer: That's correct. In Django, user sessions are managed by the `django.contrib.sessions` module. It handles user authentication, session creation, and session management. The `django.http.Session` class provides methods for creating, updating, and deleting sessions.\n",
            "Interviewee: Yes\n",
            "Interviewer: Can you explain how sessions are stored and retrieved?\n",
            "Interviewer  : Sessions are stored in the file-based database, which is a SQLite database. They are retrieved by the session file, which gets deleted when the user logs out.\n",
            "Interviewees:\n",
            "Interviewee: But you answered it yourself\n",
            "Interviewer: I did.\n",
            "Intervieweer:  What about the session cookie? How does it work?\n",
            "Interviewe: The session cookie is stored on the client-side and is sent with each request. It is used to identify the session and to retrieve the session data\n",
            "Interviewee: quit\n",
            "Interviewer: Can you tell us about Django's templating engine?\n",
            "Intervieweer  : I can. The Django templating system is built on the Jinja2 templating language. It provides a way to separate presentation logic from application logic.\n",
            "Interviewe  :\n",
            "Interviewer: Thank you for your time. This concludes the interview.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"./lora-llama-2-3b-unsupervised\"  # Path to the fine-tuned model directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Function to simulate the interview\n",
        "def interview_simulation(model, tokenizer):\n",
        "    # Initial context to set the tone for the interview\n",
        "    context = \"Interviewee: Hello, I'm ready for the interview.\\nInterviewer: Welcome! Let's start with some questions about Django and Flask.\\n\"\n",
        "\n",
        "    # First question to start the interview\n",
        "    interviewer_question = \"To start, can you describe how Django manages user sessions?\"\n",
        "    print(f\"Interviewer: {interviewer_question}\")\n",
        "    context += f\"Interviewer: {interviewer_question}\\n\"\n",
        "\n",
        "    # Start the interactive loop\n",
        "    while True:\n",
        "        torch.cuda.empty_cache()\n",
        "        # Interviewee's input\n",
        "        interviewee_response = input(\"Interviewee: \")\n",
        "        print(f\"Interviewee: {interviewee_response}\")\n",
        "        context += f\"Interviewee: {interviewee_response}\\n\"\n",
        "\n",
        "        # Prepare context for next interviewer question\n",
        "        context += \"Interviewer: \"\n",
        "\n",
        "        # Tokenize and generate the next question based on the context\n",
        "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                attention_mask=attention_mask,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7  # Control creativity for follow-ups\n",
        "            )\n",
        "\n",
        "        # Decode the generated question\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        next_interviewer_question = generated_text.split(\"Interviewer:\")[-1].strip().split(\"Interviewee:\")[0].strip()\n",
        "\n",
        "        # Print and update context\n",
        "        print(f\"Interviewer: {next_interviewer_question}\")\n",
        "        context += f\"{next_interviewer_question}\\n\"\n",
        "\n",
        "        # Exit condition for the loop\n",
        "        if interviewee_response.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Interviewer: Thank you for your time. This concludes the interview.\")\n",
        "            break\n",
        "\n",
        "# Run the interview simulation\n",
        "interview_simulation(model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the GPT-J model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset for demonstration\n",
    "dataset = [\n",
    "    {\n",
    "        \"prompt\": \"Let's start with Django. How does Django manage user sessions?\",\n",
    "        \"response\": \"Django uses a session framework to store data on a per-user basis. It generates a session ID that's stored in the user's browser, while the associated session data is stored on the server, typically in the database or a cache.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Good. Can you elaborate on how Django uses middleware in managing sessions?\",\n",
    "        \"response\": \"Sure. Middleware in Django processes requests before they reach the view and after they've been processed by the view. For sessions, Django has middleware that reads the session data from the request and loads it, then saves any changes to the session before sending the response back to the client.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"That's correct. Now, explain how the Django ORM simplifies interactions with the database.\",\n",
    "        \"response\": \"The Django ORM lets developers interact with the database using Python code instead of writing raw SQL. It provides models that map directly to database tables, which makes querying, creating, and updating records easy and database-agnostic.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What are some limitations or challenges with using Django ORM?\",\n",
    "        \"response\": \"One challenge is with complex queries. While Django ORM covers most CRUD operations well, it can become less efficient for complex joins or aggregations. In those cases, developers sometimes need to use raw SQL for better performance.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"That makes sense. Moving to Flask, what are blueprints and how would they benefit a larger application?\",\n",
    "        \"response\": \"Blueprints in Flask help organize application functionality by allowing different sections of an app to be separated into modular components. For example, in a large app, you could have separate blueprints for authentication, user management, and content, which keeps the codebase more organized and scalable.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"If we use Flask for an API-based application, how would you handle HTTP methods and route organization?\",\n",
    "        \"response\": \"In Flask, you can specify HTTP methods directly on routes by setting the `methods` argument. For example, using `@app.route('/login', methods=['POST'])` ensures that the route only accepts POST requests. Routes can be further organized by grouping them within blueprints, which keeps related routes in one place.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Let's discuss environments. How can Flask manage different environments like development and production?\",\n",
    "        \"response\": \"Flask can handle multiple environments through configuration objects or files. By setting the `FLASK_ENV` environment variable to 'development' or 'production', you can load different settings, such as enabling debug mode in development and setting specific database URIs for each environment.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can you explain Django signals and provide an example of when they'd be useful in a project?\",\n",
    "        \"response\": \"Django signals are used to trigger actions when certain events occur in the application. For instance, you might use a signal to create a user profile automatically after a new user registers. This decouples the user creation process from the profile creation process, which keeps the code modular.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Great example. Now, how would you approach file uploads in Flask?\",\n",
    "        \"response\": \"In Flask, file uploads are handled through `request.files`. First, you configure an upload folder using `app.config['UPLOAD_FOLDER']`, then access the file via `request.files['file']`. Finally, you can save it to the server using the `save` method.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Lastly, in both Django and Flask, how would you implement user authentication in a custom way?\",\n",
    "        \"response\": \"In Django, custom authentication can be done by creating a custom user model and setting `AUTH_USER_MODEL` in `settings.py`. In Flask, custom authentication typically involves creating a login route that checks credentials against a database, using JWT tokens or sessions to manage authenticated states.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to prepare dataset for training\n",
    "def prepare_dataset(dataset):\n",
    "    texts = []\n",
    "    for entry in dataset:\n",
    "        text = f\"Interviewer: {entry['prompt']}\\nCandidate: {entry['response']}\\n\"\n",
    "        texts.append(text)\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "hf_dataset = prepare_dataset(dataset)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments with Logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gptj-interview\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",         # Directory for storing logs\n",
    "    logging_steps=10,             # Log every 10 steps\n",
    "    report_to=\"tensorboard\"       # Report logs to TensorBoard\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-Tuning with Logging\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./gptj-interview\"  # Path to the fine-tuned model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to simulate the interview\n",
    "def interview_simulation(model, tokenizer):\n",
    "    # Initial context to set the tone for the interview\n",
    "    context = \"Candidate: Hello, I'm ready for the interview.\\nInterviewer: Welcome! Let's start with some questions about Django and Flask.\\n\"\n",
    "    \n",
    "    # First question to start the interview\n",
    "    interviewer_question = \"To start, can you describe how Django manages user sessions?\"\n",
    "    print(f\"Interviewer: {interviewer_question}\")\n",
    "    context += f\"Interviewer: {interviewer_question}\\n\"\n",
    "    \n",
    "    # Start the interactive loop\n",
    "    while True:\n",
    "        # Candidate's input\n",
    "        candidate_response = input(\"Candidate: \")\n",
    "        context += f\"Candidate: {candidate_response}\\n\"\n",
    "        \n",
    "        # Prepare context for next interviewer question\n",
    "        context += \"Interviewer: \"\n",
    "        \n",
    "        # Tokenize and generate the next question based on the context\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=512,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.7  # Control creativity for follow-ups\n",
    "            )\n",
    "        \n",
    "        # Decode the generated question\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        next_interviewer_question = generated_text.split(\"Interviewer:\")[-1].strip().split(\"Candidate:\")[0].strip()\n",
    "        \n",
    "        # Print and update context\n",
    "        print(f\"Interviewer: {next_interviewer_question}\")\n",
    "        context += f\"{next_interviewer_question}\\n\"\n",
    "\n",
    "        # Exit condition for the loop\n",
    "        if candidate_response.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Interviewer: Thank you for your time. This concludes the interview.\")\n",
    "            break\n",
    "\n",
    "# Run the interview simulation\n",
    "interview_simulation(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
